{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "In this notebook Sentiment Analysis will be performed - a process of assigning whether document is positive or negative.\n",
    "\n",
    "It will be done on very popular dataset of [imdb.com](imdb.com) movie reviews which can be obtained for free at http://ai.stanford.edu/~amaas/data/sentiment/ website.\n",
    "\n",
    "Solution will be divided into two cases:\n",
    "1. Simple Machine Learning models with usage of Bag of Words technique.\n",
    "2. Simple Machine Learning models with usage of TF-IDF technique.\n",
    "3. Bidirectional Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:18:31.868894Z",
     "start_time": "2020-03-09T01:18:28.376149Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading pun|kt: Package 'pun|kt' not found in index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import shutil\n",
    "import tarfile\n",
    "import itertools\n",
    "import urllib.request\n",
    "from pathos.multiprocessing import ProcessingPool as Pool\n",
    "from contextlib import contextmanager\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk\n",
    "for package in [\"pun|kt\", \"wordnet\", \"stopwords\"]:\n",
    "    nltk.download(package)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import STATUS_OK, fmin, tpe, space_eval\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import get_custom_objects\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:18:31.877880Z",
     "start_time": "2020-03-09T01:18:31.870734Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_LINK = \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "TEMP_FOLDER = \"temp\"\n",
    "GLOBAL_SEED = 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:18:31.889121Z",
     "start_time": "2020-03-09T01:18:31.879648Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_data():\n",
    "    \"\"\"\"\"\"\n",
    "    \n",
    "    # Creating empty folder\n",
    "    if not os.path.exists(TEMP_FOLDER):\n",
    "        os.mkdir(TEMP_FOLDER)\n",
    "        \n",
    "    # Downloading data to temp directory\n",
    "    filepath = os.path.join(TEMP_FOLDER, DATA_LINK.split(\"/\")[-1])\n",
    "    if not os.path.exists(filepath):\n",
    "        urllib.request.urlretrieve(DATA_LINK, filepath)\n",
    "    \n",
    "    # Unpacking data\n",
    "    with tarfile.open(filepath, \"r:gz\") as tar:\n",
    "        tar.extractall(path=TEMP_FOLDER)\n",
    "        \n",
    "    # Loading raw data\n",
    "    train_data_positive_path = os.path.join(TEMP_FOLDER, \"aclImdb\", \"train\", \"pos\")\n",
    "    train_data_negative_path = os.path.join(TEMP_FOLDER, \"aclImdb\", \"train\", \"neg\")\n",
    "    test_data_positive_path = os.path.join(TEMP_FOLDER, \"aclImdb\", \"test\", \"pos\")\n",
    "    test_data_negative_path = os.path.join(TEMP_FOLDER, \"aclImdb\", \"test\", \"neg\")\n",
    "    \n",
    "    train_data = {\"review\": [], \"sentiment\": []}\n",
    "    test_data = {\"review\": [], \"sentiment\": []}\n",
    "    \n",
    "    def _dir_to_data(directory, sentiment, data_dict):\n",
    "        \"\"\"Loads .txt files in directory and saves text in data_dict along with sentiment \n",
    "        information\n",
    "        \"\"\"\n",
    "        files = os.listdir(directory)\n",
    "        for i in tqdm(range(len(files)), desc=\"Loading foles from '{}' dir...\".format(directory)):\n",
    "            with open(os.path.join(directory, files[i]), \"r\") as f:\n",
    "                data_dict[\"review\"].append(f.read())\n",
    "                data_dict[\"sentiment\"].append(sentiment)\n",
    "                \n",
    "    _dir_to_data(train_data_positive_path, \"positive\", train_data)\n",
    "    _dir_to_data(train_data_negative_path, \"negative\", train_data)\n",
    "    _dir_to_data(test_data_positive_path, \"positive\", test_data)\n",
    "    _dir_to_data(test_data_negative_path, \"negative\", test_data)\n",
    "    \n",
    "    # Cleaning temp files\n",
    "    shutil.rmtree(TEMP_FOLDER)\n",
    "    \n",
    "    return pd.DataFrame(train_data), pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T00:10:43.893525Z",
     "start_time": "2020-03-04T00:10:43.889298Z"
    }
   },
   "source": [
    "- Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.570692Z",
     "start_time": "2020-03-09T01:18:31.890673Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading foles from 'temp/aclImdb/train/pos' dir...: 100%|██████████| 12500/12500 [00:00<00:00, 18753.73it/s]\n",
      "Loading foles from 'temp/aclImdb/train/neg' dir...: 100%|██████████| 12500/12500 [00:00<00:00, 18962.52it/s]\n",
      "Loading foles from 'temp/aclImdb/test/pos' dir...: 100%|██████████| 12500/12500 [00:00<00:00, 18758.75it/s]\n",
      "Loading foles from 'temp/aclImdb/test/neg' dir...: 100%|██████████| 12500/12500 [00:00<00:00, 19434.65it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-04T00:20:22.947346Z",
     "start_time": "2020-03-04T00:20:22.914165Z"
    }
   },
   "source": [
    "- Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.577452Z",
     "start_time": "2020-03-09T01:19:33.572832Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.661997Z",
     "start_time": "2020-03-09T01:19:33.579522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  For a movie that gets no respect there sure ar...  positive\n",
       "1  Bizarre horror movie filled with famous faces ...  positive\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...  positive\n",
       "3  It's a strange feeling to sit alone in a theat...  positive\n",
       "4  You probably all already know this by now, but...  positive"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.802734Z",
     "start_time": "2020-03-09T01:19:33.663843Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.810966Z",
     "start_time": "2020-03-09T01:19:33.804738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Based on an actual story, John Boorman shows t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This is a gem. As a Film Four production - the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I really like this show. It has drama, romance...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is the best 3-D experience Disney has at ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Of the Korean movies I've seen, only three had...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  Based on an actual story, John Boorman shows t...  positive\n",
       "1  This is a gem. As a Film Four production - the...  positive\n",
       "2  I really like this show. It has drama, romance...  positive\n",
       "3  This is the best 3-D experience Disney has at ...  positive\n",
       "4  Of the Korean movies I've seen, only three had...  positive"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.819034Z",
     "start_time": "2020-03-09T01:19:33.813253Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train reviews example:\n",
      "Would anyone really watch this RUBBISH if it didn't contain little children running around nude? From a cinematic point of view it is probably one of the worst films I have encountered absolutely dire. Some perv woke up one day and thought I will make a film with little girls in and call it art, stick them in countryside and there isn't any need for a story or explanation of how they got there or why they don't appear to live anywhere or have parents because p*rn films don't need anything like that. I would comment on the rest of the film but I haven't ticked spoilers so I will just say avoid, avoid avoid and find yourself a proper film to watch\n",
      "\n",
      "A horror movie is being shot and things aren't going well. It's about a masked killer. The director tells off the killer in front of the cast and crew. He goes crazy and kills two people. He's killed himself and the film is never finished. Twelve years later a bunch of film students decide to try and finish it--but there's a curse. People who try and finish it are killed themselves. The students ignore that. Guess what happens next?<br /><br />The plot is old hat but this isn't bad...for what it is (a low budget slasher film). It's well-made with a young and fairly talented young cast. No one is great but no one is terrible either. It also avoids the obligatory (and needless) female nude scenes. It moves quickly, the gore is nice and bloody and the script doesn't insult your intelligence. Also Molly Ringwald is in this having the time of her life playing a bitchy faded actress.<br /><br />No great shakes but not bad at all. I give it a 7.\n",
      "\n",
      "--- Test reviews example:\n",
      "The Academy Award winning 'Kramer vs. Kramer' follows a snazzy businessman Ted Kramer (Dustin Hoffman) and his divorce with his bored wife (Meryl Streep). One day Ted's wife leaves him and their child in search for a better life, forcing Ted to become closer to his son (Justin Henry). The two bond and become very close (but only after some friction), and just as everything is going perfect Ted's wife comes back to town and wants sole custody of their son. Ted then goes on a mission not to let his son get taken away from him, and fights his wife in court. Dustin Hoffman gives a tremendous performance along with Meryl Streep, and young Justin Henry is impressive. It's a sad emotional roller-coaster of a movie, but it's a very well-made and inspiring film. The film took the Oscar for Best Picture at the 1979 Academy Awards, along with Best Actor for Hoffman and Best Supporting Actress for Streep. If you don't mind a tearjerker, 'Kramer vs. Kramer' is a great film to watch. Grade: B+\n",
      "\n",
      "Oh God. Why is it that Nickelodeon has such a hard time producing even a half-decent movie? I mean, this movie might have been good, but it was:<br /><br />A. Too short B. Rather superficial, stereotypical, and insulting to some C. Ultimately pointless<br /><br />First of all, the \"dress up the nerd to look cool\" thing was VERY consumerist, VERY superficial, VERY pointless, and VERY insulting. It has the stereotypical nerds-stupid faces, glasses, never kissed, vacations with his mom, etc. Well maybe the reason that guy has never kissed a girl is because he's gay! Does that mean that all gays are nerds? And what's wrong with being friends with your mother?<br /><br />The worst part, by far, was the ending. The whole drama of the movie revolved around Zoey finding out Chase loved her, and blah blah blah, and then, when Chase finally decided to tell her, <br /><br />A. he didn't tell her in person because right as he was about to the typical distraction came along B. he tried to text message her, but her PDA fell into a fountain and died before she got the message.<br /><br />The End.<br /><br />HOW LAME IS THAT????? I mean, why is it that cartoonists just can't change anything in the series? So many of us would like to see these two get together. Why can't we see it? I mean, are the producers really that uncreative, that they can't think up new problems to go with changes in the series? So they have to stick with the same plot and outlines, and make as many episodes as they can just using those? After a while, it gets dull and frustrating.<br /><br />I WANTED TO SEE SOME ACTUAL ROMANCE IN HERE, DARN IT!<br /><br />Okay. I'm done with my rant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews_to_show = 2\n",
    "\n",
    "print(\"--- Train reviews example:\")\n",
    "for review_id in random.sample(range(len(df_train)), reviews_to_show):\n",
    "    print(df_train[\"review\"].iloc[review_id], end=\"\\n\\n\")\n",
    "    \n",
    "print(\"--- Test reviews example:\")\n",
    "for review_id in random.sample(range(len(df_test)), reviews_to_show):\n",
    "    print(df_test[\"review\"].iloc[review_id], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Creating Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:19:33.829451Z",
     "start_time": "2020-03-09T01:19:33.820543Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(document, lemmatizer=WordNetLemmatizer(), stemmer=PorterStemmer()):\n",
    "    \"\"\"Operation of dividing single document into list of tokens. Tokens are cleaned by:\n",
    "        1. Removing HTML from text.\n",
    "        2. Casting words to lowercase.\n",
    "        3. Removing punctuation.\n",
    "        4. Removing words containing digits and digits.\n",
    "        5. Removing stopwords.\n",
    "        6. Removal of tokens shorter than 3 letters.\n",
    "        8. Lemmatization.\n",
    "        9. Stemming.\n",
    "    \"\"\"\n",
    "    \n",
    "    document_cleaned = BeautifulSoup(document, \"html.parser\").get_text()\n",
    "    document_cleaned = document.lower()\n",
    "    document_cleaned = re.sub(r\"[^a-zA-Z0-9]\", \" \", document_cleaned)\n",
    "    document_cleaned = re.sub(r\"\\w*\\d\\w*\", \"\", document_cleaned)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for token in word_tokenize(document_cleaned):\n",
    "        if token not in stopwords.words(\"english\") and len(token) > 3:\n",
    "            token = stemmer.stem(token)\n",
    "            token = lemmatizer.lemmatize(token)\n",
    "            tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def tokenize_corpus(corpus, tokenize_func=tokenize, workers_num=12):\n",
    "    \"\"\"Wrapper on 'tokenize' function that accelerates preprocessing via multiprocessing.\"\"\"   \n",
    "    \n",
    "    def _process_chunk(chunks):        \n",
    "        document_tokens = []\n",
    "        for i, document in enumerate(chunks):\n",
    "            document_tokens.append(tokenize_func(document))\n",
    "            \n",
    "        return document_tokens\n",
    "    \n",
    "    corpus_size = len(corpus)\n",
    "    chunk_size = int(np.ceil(corpus_size / workers_num))\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(workers_num):\n",
    "        chunk_lower_th, chunk_higher_th = i * chunk_size, (i + 1) * chunk_size\n",
    "        chunk_higher_th = corpus_size if chunk_higher_th >= corpus_size else chunk_higher_th\n",
    "        chunks.append(corpus[chunk_lower_th:chunk_higher_th])\n",
    "            \n",
    "    with Pool(workers_num) as pool:\n",
    "        sys.stdout.flush()\n",
    "        results = pool.map(_process_chunk, chunks)\n",
    "        \n",
    "    return sum(results, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:03.793725Z",
     "start_time": "2020-03-09T01:19:33.830722Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train reviews...\n",
      "\n",
      "Preprocessing test reviews...\n",
      "\n",
      "Creating train targets...\n",
      "\n",
      "Creating test targets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing train reviews...\")\n",
    "train_tokens = tokenize_corpus(df_train[\"review\"])\n",
    "\n",
    "print(\"\\nPreprocessing test reviews...\")\n",
    "test_tokens = tokenize_corpus(df_test[\"review\"])\n",
    "\n",
    "print(\"\\nCreating train targets...\")\n",
    "train_targets = (df_train[\"sentiment\"] == \"positive\").astype(int)\n",
    "\n",
    "print(\"\\nCreating test targets...\")\n",
    "test_targets = (df_test[\"sentiment\"] == \"positive\").astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:03.799124Z",
     "start_time": "2020-03-09T01:26:03.795654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['clair', 'deni', 'demonstr', 'repeatedli', 'film', 'need', 'tell', 'stori', 'suffici', 'creat', 'experi', 'allow', 'viewer', 'take', 'ingredi', 'make', 'ostens', 'idea', 'within', 'framework', 'linear', 'film', 'older', 'live', 'french', 'swiss', 'border', 'devot', 'dog', 'still', 'lover', 'whose', 'cardiac', 'statu', 'increasingli', 'threaten', 'life', 'littl', 'famili', 'infrequ', 'meet', 'discov', 'need', 'heart', 'transplant', 'opt', 'go', 'tahiti', 'japan', 'obtain', 'heart', 'transplant', 'black', 'market', 'rekindl', 'long', 'lost', 'relationship', 'form', 'tahitian', 'woman', 'year', 'deni', 'outlin', 'stori', 'camera', 'explor', 'loneli', 'soul', 'vast', 'natur', 'interact', 'peopl', 'anim', 'much', 'time', 'film', 'make', 'sen', 'hard', 'connect', 'dot', 'laid', 'beauti', 'pictur', 'life', 'sort', 'like', 'look', 'observ', 'integr', 'process', 'make', 'use', 'form', 'film', 'make', 'much', 'strang', 'beauti', 'beau', 'travail', 'clair', 'deni', 'develop', 'signatur', 'techniqu', 'whether', 'viewer', 'find', 'finish', 'product', 'reward', 'much', 'individu', 'method', 'process', 'visual', 'conceptu', 'inform', 'interest', 'visual', 'captiv', 'film', 'mani', 'viewer', 'find', 'overli', 'long', 'discours', 'littl', 'perhap', 'watch', 'chang', 'gradi', 'harp']\n",
      "['grew', 'watch', 'scoobi', 'forev', 'cartoon', 'move', 'away', 'routin', 'bore', 'viewer', 'crook', 'mask', 'routin', 'get', 'cartoon', 'chang', 'meant', 'replac', 'scoobi', 'gang', 'break', 'crime', 'scene', 'viewer', 'writer', 'sure', 'cartoon', 'focu', 'scoobi', 'shaggi', 'inherit', 'larg', 'money', 'money', 'thwart', 'world', 'conquest', 'plan', 'scientist', 'goon', 'small', 'homag', 'gang', 'gang', 'featur', 'time', 'time', 'scoobi', 'still', 'appreci', 'bond', 'talk', 'along', 'joke', 'come', 'enjoyth', 'cartoon', 'support', 'creator', 'writer', 'produc', 'last', 'scoobi', 'cartoon']\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens[112])\n",
    "print(train_tokens[522])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at tokens connected to \"positive\" and \"negative\" reviews. Previewing tokens after ignoring start of both most common token lists as those are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:04.185809Z",
     "start_time": "2020-03-09T01:26:03.800594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review tokens:\n",
      "\t0. ('final', 1717)\n",
      "\t1. ('episod', 1698)\n",
      "\t2. ('effect', 1683)\n",
      "\t3. ('around', 1674)\n",
      "\t4. ('without', 1672)\n",
      "\t5. ('last', 1663)\n",
      "\t6. ('use', 1647)\n",
      "\t7. ('tell', 1640)\n",
      "\t8. ('place', 1615)\n",
      "\t9. ('entertain', 1606)\n",
      "\t10. ('need', 1604)\n",
      "\t11. ('help', 1598)\n",
      "\t12. ('happen', 1592)\n",
      "\t13. ('done', 1586)\n",
      "\t14. ('especi', 1576)\n",
      "\t15. ('almost', 1566)\n",
      "\t16. ('child', 1558)\n",
      "\t17. ('begin', 1556)\n",
      "\t18. ('moment', 1554)\n",
      "\t19. ('pretti', 1550)\n",
      "\n",
      "Negative review tokens:\n",
      "\t0. ('mean', 1738)\n",
      "\t1. ('music', 1737)\n",
      "\t2. ('right', 1717)\n",
      "\t3. ('action', 1709)\n",
      "\t4. ('shot', 1685)\n",
      "\t5. ('comedi', 1678)\n",
      "\t6. ('use', 1674)\n",
      "\t7. ('sure', 1665)\n",
      "\t8. ('might', 1658)\n",
      "\t9. ('aw', 1657)\n",
      "\t10. ('idea', 1654)\n",
      "\t11. ('quit', 1650)\n",
      "\t12. ('money', 1643)\n",
      "\t13. ('role', 1642)\n",
      "\t14. ('laugh', 1639)\n",
      "\t15. ('kind', 1631)\n",
      "\t16. ('call', 1618)\n",
      "\t17. ('without', 1596)\n",
      "\t18. ('terribl', 1590)\n",
      "\t19. ('expect', 1588)\n"
     ]
    }
   ],
   "source": [
    "positive_train_tokens = Counter()\n",
    "negative_train_tokens = Counter()\n",
    "for document_tokens, is_pos in zip(train_tokens, df_train[\"sentiment\"] == \"positive\"):\n",
    "    if is_pos:\n",
    "        positive_train_tokens.update(document_tokens)\n",
    "    else:\n",
    "        negative_train_tokens.update(document_tokens)\n",
    "        \n",
    "print(\"Positive review tokens:\")\n",
    "for i, token_count in enumerate(positive_train_tokens.most_common()[100:120]):\n",
    "    print(\"\\t{}. {}\".format(i, token_count))\n",
    "    \n",
    "print(\"\\nNegative review tokens:\")\n",
    "for i, token_count in enumerate(negative_train_tokens.most_common()[100:120]):\n",
    "    print(\"\\t{}. {}\".format(i, token_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of the reviews - Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already preprocessed tokens with `tokenize_corpus` are used. `CountVectorizer` doesn't support `njobs` parameter that allows multiprocessing therefore it's slow. Created `tokenize_corpus` function preprocess whole corpus about 6 times faster than single for loop (on my current machine). \n",
    "\n",
    "By preprocessing data once, different kind of `CountVectorizers` can be created very fast. If `tokenize` functian was used as `tokenizer` each `CountVectorizer` would need to preprocess whole corpus from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizer code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:04.189497Z",
     "start_time": "2020-03-09T01:26:04.187152Z"
    }
   },
   "outputs": [],
   "source": [
    "def return_tokens(token_list):\n",
    "    \"\"\"Proxy function so CountVectorizer can use already preprocessed corpus where each document\n",
    "    is a list of tokens.\n",
    "    \"\"\"\n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of words - no ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:05.236175Z",
     "start_time": "2020-03-09T01:26:04.190873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=0.75, max_features=15000, min_df=15,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function return_tokens at 0x15687fae8>, vocabulary=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer(\n",
    "    analyzer=\"word\", \n",
    "    min_df=15, \n",
    "    max_df=0.75, \n",
    "    max_features=15000,\n",
    "    lowercase=False,\n",
    "    tokenizer=return_tokens\n",
    ")\n",
    "\n",
    "bow.fit(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:05.247697Z",
     "start_time": "2020-03-09T01:26:05.237705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 9779\n"
     ]
    }
   ],
   "source": [
    "print(\"Dictionary size: {}\".format(len(bow.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of words - ngrams (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:14.655191Z",
     "start_time": "2020-03-09T01:26:05.250782Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=0.75, max_features=25000, min_df=15,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function return_tokens at 0x15687fae8>, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_ngrams_12 = CountVectorizer(\n",
    "    analyzer=\"word\", \n",
    "    min_df=15, \n",
    "    max_df=0.75, \n",
    "    ngram_range=(1,2),\n",
    "    max_features=25000,\n",
    "    lowercase=False,\n",
    "    tokenizer=return_tokens\n",
    ")\n",
    "\n",
    "bow_ngrams_12.fit(train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:14.682677Z",
     "start_time": "2020-03-09T01:26:14.657533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 23474\n"
     ]
    }
   ],
   "source": [
    "print(\"Dictionary size: {}\".format(len(bow_ngrams_12.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF no ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:15.672294Z",
     "start_time": "2020-03-09T01:26:14.684383Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf = TfidfTransformer()\n",
    "tf_idf.fit(bow.transform(train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF - ngrams(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:18.511745Z",
     "start_time": "2020-03-09T01:26:15.673855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_ngrams_12 = TfidfTransformer()\n",
    "tf_idf_ngrams_12.fit(bow_ngrams_12.transform(train_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case DummyClassifier will be used as baseline and MultinominalNB (for Bag of Words), GaussianNB(for TF-IDF), XGBoostClassifier, RandomForestClassifier as a good model candidate.\n",
    "\n",
    "Wrappers `BowBasedModel` and `TfIdfBasedModel` are created to simplyfy loop which is seeking combination with highest success probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T01:26:18.524074Z",
     "start_time": "2020-03-09T01:26:18.512919Z"
    }
   },
   "outputs": [],
   "source": [
    "class BowBasedModel:\n",
    "    \"\"\"Helper class that handles Bag of Word vectorization during training and prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, bow_ensembling, normalize=False):\n",
    "        self.bow_ensembling = bow_ensembling\n",
    "        self.model = model\n",
    "        self.normalizer = MaxAbsScaler()\n",
    "        self.normalize = normalize\n",
    "        self.train_score = None\n",
    "        self.val_score = None\n",
    "        \n",
    "    def fit(self, X, y, val_size=0.2):\n",
    "        \"\"\"Process of dividing train data into train and val datasets. At start data is transformed \n",
    "        with Bag of Words ensembler. Then data is divided and shuffled in stratified way. Errors are \n",
    "        stored in local fields of class.\n",
    "        \"\"\"\n",
    "        X_vectorized = self.bow_ensembling.transform(X)\n",
    "        X_vectorized = X_vectorized.toarray()\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_vectorized, y, test_size=val_size, random_state=GLOBAL_SEED, stratify=y)\n",
    "        \n",
    "        if self.normalize:\n",
    "            X_train = self.normalizer.fit_transform(X_train)\n",
    "            X_val = self.normalizer.transform(X_val)\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        train_pred = self.model.predict(X_train)\n",
    "        self.train_score = accuracy_score(y_train, train_pred)\n",
    "            \n",
    "        val_pred = self.model.predict(X_val)\n",
    "        self.val_score = accuracy_score(y_val, val_pred)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Uses model to return prediction from preprocessed data.\"\"\"\n",
    "        X_input = self.bow_ensembling.transform(X)\n",
    "        X_input = elf.normalizer.transform(X_input) if self.normalize else X_input\n",
    "        return model.predict(X_input)\n",
    "            \n",
    "class TfIdfBasedModel:\n",
    "    \"\"\"Helper class that handles Bag of Word and TF-IDF vectorization during training and \n",
    "    prediction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, bow_ensembling, tf_idf_ensembling):\n",
    "        self.bow_ensembling = bow_ensembling\n",
    "        self.tf_idf_ensembling = tf_idf_ensembling\n",
    "        self.model = model\n",
    "        self.train_score = None\n",
    "        self.val_score = None\n",
    "        \n",
    "    def fit(self, X, y, val_size=0.2):\n",
    "        \"\"\"Process of dividing train data into train and val datasets. At start data is transformed \n",
    "        with Bag of Words ensembler. Then data is divided and shuffled in stratified way. Addionaly \n",
    "        data can be normalized if requested in class constructor. Errors are stored in local fields \n",
    "        of class.\n",
    "        \"\"\"\n",
    "        X_vectorized = self.bow_ensembling.transform(X)\n",
    "        X_vectorized = self.tf_idf_ensembling.transform(X_vectorized)\n",
    "        X_vectorized = X_vectorized.toarray()\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_vectorized, y, test_size=val_size, random_state=GLOBAL_SEED, stratify=y)\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        train_pred = self.model.predict(X_train)\n",
    "        self.train_score = accuracy_score(y_train, train_pred)\n",
    "            \n",
    "        val_pred = self.model.predict(X_val)\n",
    "        self.val_score = accuracy_score(y_val, val_pred)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Uses model to return prediction on preprocessed data.\"\"\"\n",
    "        X_input = self.bow_ensembling.transform(X)\n",
    "        X_input = self.tf_idf_ensembling.transform(X)\n",
    "        X_input = self.scaler.transform(X_input) if self.normalize else X_input\n",
    "        return model.predict(X_input)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeking for best model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T02:26:46.903542Z",
     "start_time": "2020-03-09T01:26:18.525844Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kamilkrzyk/.pyenv/versions/3.6.6/envs/udacity-workspace/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/kamilkrzyk/.pyenv/versions/3.6.6/envs/udacity-workspace/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/kamilkrzyk/.pyenv/versions/3.6.6/envs/udacity-workspace/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/kamilkrzyk/.pyenv/versions/3.6.6/envs/udacity-workspace/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "bow_labels = [\"Bag of Words\", \"Bag of Words (ngrams)\"]\n",
    "bow_variants = [bow, bow_ngrams_12]\n",
    "\n",
    "tfidf_labels = [\"TF-IDF\", \"TF-IDF (ngrams)\"]\n",
    "tf_idf_variants = [tf_idf, tf_idf_ngrams_12]\n",
    "\n",
    "bow_models = [DummyClassifier, RandomForestClassifier, MultinomialNB, XGBClassifier]\n",
    "tf_idf_models = [DummyClassifier, RandomForestClassifier, GaussianNB, XGBClassifier]\n",
    "\n",
    "normalize = [True, False, False, True]\n",
    "\n",
    "results = {\n",
    "    \"Model Name\": [],\n",
    "    \"Ensemble Type\": [],\n",
    "    \"Train Accuracy\": [],\n",
    "    \"Val Accuracy\": []\n",
    "}\n",
    "\n",
    "for bow_label, bow_variant in zip(bow_labels, bow_variants):\n",
    "    for model_class, norm in zip(bow_models, normalize):\n",
    "        model = BowBasedModel(model_class(), bow_variant, normalize=norm)\n",
    "        model.fit(train_tokens, train_targets)\n",
    "        \n",
    "        results[\"Model Name\"].append(model_class.__name__)\n",
    "        results[\"Ensemble Type\"].append(bow_label)\n",
    "        results[\"Train Accuracy\"].append(model.train_score)\n",
    "        results[\"Val Accuracy\"].append(model.val_score)\n",
    "    \n",
    "bow_tf_idf_pairs = zip(zip(bow_labels, bow_variants), zip(tfidf_labels, tf_idf_variants))\n",
    "for (bow_label, bow_variant), (tfidf_label, tfidf_variant) in bow_tf_idf_pairs:\n",
    "    for model_class in tf_idf_models:\n",
    "        model = TfIdfBasedModel(model_class(), bow_variant, tfidf_variant)\n",
    "        model.fit(train_tokens, train_targets)\n",
    "\n",
    "        results[\"Model Name\"].append(model_class.__name__)\n",
    "        results[\"Ensemble Type\"].append(\"{}, {}\".format(bow_label, tfidf_label))\n",
    "        results[\"Train Accuracy\"].append(model.train_score)\n",
    "        results[\"Val Accuracy\"].append(model.val_score)\n",
    "            \n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T02:26:46.985383Z",
     "start_time": "2020-03-09T02:26:46.922511Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Ensemble Type</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Val Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>Bag of Words (ngrams)</td>\n",
       "      <td>0.89590</td>\n",
       "      <td>0.8656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MultinomialNB</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.87160</td>\n",
       "      <td>0.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Bag of Words (ngrams)</td>\n",
       "      <td>0.93665</td>\n",
       "      <td>0.8462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Bag of Words (ngrams), TF-IDF (ngrams)</td>\n",
       "      <td>0.95390</td>\n",
       "      <td>0.8440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.93690</td>\n",
       "      <td>0.8420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>Bag of Words, TF-IDF</td>\n",
       "      <td>0.95355</td>\n",
       "      <td>0.8392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>Bag of Words (ngrams), TF-IDF (ngrams)</td>\n",
       "      <td>0.93250</td>\n",
       "      <td>0.8244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Bag of Words (ngrams), TF-IDF (ngrams)</td>\n",
       "      <td>0.99235</td>\n",
       "      <td>0.7874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Bag of Words (ngrams)</td>\n",
       "      <td>0.99425</td>\n",
       "      <td>0.7740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Bag of Words, TF-IDF</td>\n",
       "      <td>0.99320</td>\n",
       "      <td>0.7710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.99370</td>\n",
       "      <td>0.7690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>Bag of Words, TF-IDF</td>\n",
       "      <td>0.85630</td>\n",
       "      <td>0.7626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>Bag of Words (ngrams), TF-IDF (ngrams)</td>\n",
       "      <td>0.50130</td>\n",
       "      <td>0.5066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>Bag of Words (ngrams)</td>\n",
       "      <td>0.50085</td>\n",
       "      <td>0.5022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>Bag of Words, TF-IDF</td>\n",
       "      <td>0.49785</td>\n",
       "      <td>0.4992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DummyClassifier</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.49940</td>\n",
       "      <td>0.4902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Model Name                           Ensemble Type  \\\n",
       "6            MultinomialNB                   Bag of Words (ngrams)   \n",
       "2            MultinomialNB                            Bag of Words   \n",
       "7            XGBClassifier                   Bag of Words (ngrams)   \n",
       "15           XGBClassifier  Bag of Words (ngrams), TF-IDF (ngrams)   \n",
       "3            XGBClassifier                            Bag of Words   \n",
       "11           XGBClassifier                    Bag of Words, TF-IDF   \n",
       "14              GaussianNB  Bag of Words (ngrams), TF-IDF (ngrams)   \n",
       "13  RandomForestClassifier  Bag of Words (ngrams), TF-IDF (ngrams)   \n",
       "5   RandomForestClassifier                   Bag of Words (ngrams)   \n",
       "9   RandomForestClassifier                    Bag of Words, TF-IDF   \n",
       "1   RandomForestClassifier                            Bag of Words   \n",
       "10              GaussianNB                    Bag of Words, TF-IDF   \n",
       "12         DummyClassifier  Bag of Words (ngrams), TF-IDF (ngrams)   \n",
       "4          DummyClassifier                   Bag of Words (ngrams)   \n",
       "8          DummyClassifier                    Bag of Words, TF-IDF   \n",
       "0          DummyClassifier                            Bag of Words   \n",
       "\n",
       "    Train Accuracy  Val Accuracy  \n",
       "6          0.89590        0.8656  \n",
       "2          0.87160        0.8500  \n",
       "7          0.93665        0.8462  \n",
       "15         0.95390        0.8440  \n",
       "3          0.93690        0.8420  \n",
       "11         0.95355        0.8392  \n",
       "14         0.93250        0.8244  \n",
       "13         0.99235        0.7874  \n",
       "5          0.99425        0.7740  \n",
       "9          0.99320        0.7710  \n",
       "1          0.99370        0.7690  \n",
       "10         0.85630        0.7626  \n",
       "12         0.50130        0.5066  \n",
       "4          0.50085        0.5022  \n",
       "8          0.49785        0.4992  \n",
       "0          0.49940        0.4902  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values(by=\"Val Accuracy\", ascending=False).head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed the best results are on model:\n",
    "    - `Multinominal Naive Bayes` with `Bag of Words using (1,2) range ngrams`\n",
    "\n",
    "Second best result belongs to:\n",
    "    - `XGboost Classifier` also with `Bag of Words using (1,2) range ngrams`\n",
    "    \n",
    "There is not much parameters inside of Naive Bayes model. XGBoost was used without any hyperparameters and difference between val accuracy is around ~2%. Therefore after tuning XGBoost might surpass Naive Bayes result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning XgboostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T02:26:47.013894Z",
     "start_time": "2020-03-09T02:26:46.988062Z"
    }
   },
   "outputs": [],
   "source": [
    "def cast_param_dict_values_to_int(param_dict, param_names):\n",
    "    \"\"\"Function that casts all values in sent dict to integer.\"\"\"\n",
    "    for k, v in param_dict.items():\n",
    "        if any(param in k for param in param_names):\n",
    "            param_dict[k] = int(v)\n",
    "    return param_dict\n",
    "\n",
    "@contextmanager\n",
    "def timer(description):\n",
    "    \"\"\"Context manager that returns time of the operation performed in the context scope.\"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.2f}s\".format(description, time.time() - t0))\n",
    "\n",
    "def _score(params):\n",
    "    \"\"\"Scoring function for hyperopt. Here model candidate is created and it's performance is sent\n",
    "    back to hyperopt. It will pick model parameters with the best score.\n",
    "    \"\"\"\n",
    "    params = cast_param_dict_values_to_int(params, param_names=[\"max_depth\", \"n_estimators\"])\n",
    "    \n",
    "    with timer(\"Hyperopt: Buiilt XGBoost candidate model.\"):\n",
    "        candidate = BowBasedModel(XGBClassifier(**params), bow_ngrams_12, normalize=True)\n",
    "        candidate.fit(train_tokens, train_targets)\n",
    "        \n",
    "    print(\"\\t- train accuracy: {}\".format(candidate.train_score))\n",
    "    print(\"\\t- val accuracy: {}\".format(candidate.val_score))\n",
    "\n",
    "    return {\"loss\": 1.0 - candidate.val_score, \"status\": STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T04:01:22.756397Z",
     "start_time": "2020-03-09T02:26:47.016677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperopt: Buiilt XGBoost candidate model. - done in 200.76s\n",
      "\t- train accuracy: 0.88555                            \n",
      "\t- val accuracy: 0.84                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 230.73s                         \n",
      "\t- train accuracy: 0.89585                                                          \n",
      "\t- val accuracy: 0.8388                                                             \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 347.26s                         \n",
      "\t- train accuracy: 0.93385                                                          \n",
      "\t- val accuracy: 0.8638                                                             \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 341.49s                         \n",
      "\t- train accuracy: 0.90745                                             \n",
      "\t- val accuracy: 0.854                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 338.73s            \n",
      "\t- train accuracy: 0.8608                                              \n",
      "\t- val accuracy: 0.839                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 343.56s            \n",
      "\t- train accuracy: 0.9455                                              \n",
      "\t- val accuracy: 0.8598                                                \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 384.85s            \n",
      "\t- train accuracy: 0.9663                                              \n",
      "\t- val accuracy: 0.8584                                                \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 266.66s            \n",
      "\t- train accuracy: 0.9059                                              \n",
      "\t- val accuracy: 0.8404                                                \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 247.38s            \n",
      "\t- train accuracy: 0.9121                                              \n",
      "\t- val accuracy: 0.8524                                                \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 201.45s            \n",
      "\t- train accuracy: 0.8242                                            \n",
      "\t- val accuracy: 0.8104                                              \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 259.07s           \n",
      "\t- train accuracy: 0.89845                                            \n",
      "\t- val accuracy: 0.8548                                               \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 239.23s           \n",
      "\t- train accuracy: 0.9219                                             \n",
      "\t- val accuracy: 0.8568                                               \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 316.68s           \n",
      "\t- train accuracy: 0.93545                                              \n",
      "\t- val accuracy: 0.8624                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 205.67s             \n",
      "\t- train accuracy: 0.87875                                              \n",
      "\t- val accuracy: 0.8438                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 205.55s             \n",
      "\t- train accuracy: 0.8387                                               \n",
      "\t- val accuracy: 0.8188                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 202.49s             \n",
      "\t- train accuracy: 0.88935                                              \n",
      "\t- val accuracy: 0.842                                                  \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 317.12s             \n",
      "\t- train accuracy: 0.8223                                               \n",
      "\t- val accuracy: 0.8064                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 379.19s             \n",
      "\t- train accuracy: 0.99255                                              \n",
      "\t- val accuracy: 0.8608                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 274.02s             \n",
      "\t- train accuracy: 0.9514                                               \n",
      "\t- val accuracy: 0.8564                                                 \n",
      "Hyperopt: Buiilt XGBoost candidate model. - done in 361.01s             \n",
      "\t- train accuracy: 0.97135                                              \n",
      "\t- val accuracy: 0.8634                                                 \n",
      "100%|██████████| 20/20 [1:34:35<00:00, 283.77s/trial, best loss: 0.1362]\n"
     ]
    }
   ],
   "source": [
    "xgboost_hyperparameter_space = {\n",
    "    \"n_estimators\": hp.quniform(\"n_estimators\", 100, 300, 5),\n",
    "    \"eta\": hp.quniform(\"eta\", 0.025, 0.5, 0.025),\n",
    "    \"max_depth\": hp.quniform(\"max_depth\", 1, 8, 1),\n",
    "    \"min_child_weight\": hp.quniform(\"min_child_weight\", 1, 6, 1),\n",
    "    \"subsample\": hp.quniform(\"subsample\", 0.5, 1, 0.05),\n",
    "    \"reg_alpha\": hp.quniform(\"reg_alpha\", 0.0, 2, 0.2),\n",
    "    \"reg_lambda\": hp.quniform(\"reg_lambda\", 0.0, 2, 0.2),\n",
    "    \"gamma\": hp.quniform(\"gamma\", 0.5, 1, 0.05),\n",
    "    \"colsample_bytree\": hp.quniform(\"colsample_bytree\", 0.5, 1, 0.05),\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"predictor\": \"cpu_predictor\",\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"silent\": 1,\n",
    "    \"random_state\": GLOBAL_SEED\n",
    "}\n",
    "\n",
    "top_params = fmin(_score, xgboost_hyperparameter_space, algo=tpe.suggest, max_evals=20)\n",
    "top_params = space_eval(xgboost_hyperparameter_space, top_params)\n",
    "top_params = cast_param_dict_values_to_int(top_params, param_names=[\"max_depth\", \"n_estimators\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Constructing top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T04:08:13.302531Z",
     "start_time": "2020-03-09T04:01:22.772869Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=0.75, eta=0.325,\n",
       "       gamma=0.9500000000000001, gpu_id=-1, importance_type='gain',\n",
       "       interaction_constraints=None, learning_rate=0.324999988,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=6.0, missing=nan,\n",
       "       monotone_constraints=None, n_estimators=215, n_jobs=0,\n",
       "       num_parallel_tree=1, objective='binary:logistic',\n",
       "       predictor='cpu_predictor', random_state=2020, reg_alpha=0.8,\n",
       "       reg_lambda=1.4000000000000001, scale_pos_weight=1, silent=1,\n",
       "       subsample=0.9500000000000001, tree_method='hist',\n",
       "       validate_parameters=False, verbosity=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = MaxAbsScaler()\n",
    "\n",
    "X_vectorized = bow_ngrams_12.transform(train_tokens)\n",
    "X_vectorized = X_vectorized.toarray()\n",
    "X_vectorized = normalizer.fit_transform(X_vectorized)\n",
    "\n",
    "model = XGBClassifier(**top_params)\n",
    "model.fit(X_vectorized, train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T04:08:38.429650Z",
     "start_time": "2020-03-09T04:08:13.307648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test dataset: 0.8654\n"
     ]
    }
   ],
   "source": [
    "X_test_vectorized = bow_ngrams_12.transform(test_tokens)\n",
    "X_test_vectorized = X_test_vectorized.toarray()\n",
    "X_test_vectorized = normalizer.transform(X_test_vectorized)\n",
    "\n",
    "test_predictions = model.predict(X_test_vectorized)\n",
    "test_accuracy = accuracy_score(test_targets, test_predictions)\n",
    "print(\"Accuracy on test dataset: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Creating Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:12:11.409847Z",
     "start_time": "2020-03-09T08:12:11.401173Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_document(document, lemmatizer=WordNetLemmatizer()):\n",
    "    \"\"\"Operation of cleaning document text by:\n",
    "        1. Removing HTML from text.\n",
    "        2. Removing punctuation.\n",
    "        3. Casting words to lowercase.\n",
    "        4. Lematization.\n",
    "        5. Removing stopwords.\n",
    "    \"\"\"\n",
    "    \n",
    "    document = BeautifulSoup(document, \"html.parser\").get_text()\n",
    "    document = re.sub(r\"[^\\w\\s]\", \"\", document, re.UNICODE)\n",
    "    document = document.lower()\n",
    "    document = [lemmatizer.lemmatize(token) for token in document.split(\" \")]\n",
    "    document = [lemmatizer.lemmatize(token, \"v\") for token in document]\n",
    "    document = [token for token in document if not token in stopwords.words(\"english\")]\n",
    "    document = \" \".join(document)\n",
    "    return document\n",
    "\n",
    "def clean_corpus(corpus, clean_func=clean_document, workers_num=12):\n",
    "    \"\"\"Wrapper on 'clean_document' function that accelerates preprocessing via multiprocessing.\"\"\"   \n",
    "    \n",
    "    def _process_chunk(chunks):        \n",
    "        document_tokens = []\n",
    "        for i, document in enumerate(chunks):\n",
    "            document_tokens.append(clean_func(document))\n",
    "            \n",
    "        return document_tokens\n",
    "    \n",
    "    corpus_size = len(corpus)\n",
    "    chunk_size = int(np.ceil(corpus_size / workers_num))\n",
    "    \n",
    "    chunks = []\n",
    "    for i in range(workers_num):\n",
    "        chunk_lower_th, chunk_higher_th = i * chunk_size, (i + 1) * chunk_size\n",
    "        chunk_higher_th = corpus_size if chunk_higher_th >= corpus_size else chunk_higher_th\n",
    "        chunks.append(corpus[chunk_lower_th:chunk_higher_th])\n",
    "            \n",
    "    with Pool(workers_num) as pool:\n",
    "        sys.stdout.flush()\n",
    "        results = pool.map(_process_chunk, chunks)\n",
    "        \n",
    "    return sum(results, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:17:38.601982Z",
     "start_time": "2020-03-09T08:12:12.283604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning train reviews...\n",
      "\n",
      "Cleaning test reviews...\n",
      "\n",
      "Creating train targets...\n",
      "\n",
      "Creating test targets...\n",
      "\n",
      "Splitting train data to val dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning train reviews...\")\n",
    "train_reviews_cleaned = clean_corpus(df_train[\"review\"])\n",
    "\n",
    "print(\"\\nCleaning test reviews...\")\n",
    "test_reviews_cleaned = clean_corpus(df_test[\"review\"])\n",
    "\n",
    "print(\"\\nCreating train targets...\")\n",
    "train_targets_seq = (df_train[\"sentiment\"] == \"positive\").astype(int)\n",
    "\n",
    "print(\"\\nCreating test targets...\")\n",
    "test_targets_seq = (df_test[\"sentiment\"] == \"positive\").astype(int)\n",
    "\n",
    "print(\"\\nSplitting train data to val dataset...\")\n",
    "train_reviews_cleaned, val_reviews_cleaned, train_targets_seq, val_targets_seq = train_test_split(\n",
    "    train_reviews_cleaned, train_targets_seq, test_size=0.2, \n",
    "    random_state=GLOBAL_SEED, stratify=train_targets_seq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:17:45.653274Z",
     "start_time": "2020-03-09T08:17:38.603531Z"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_num = 10000\n",
    "keras_tokenizer = Tokenizer(num_words=vocabulary_num)\n",
    "keras_tokenizer.fit_on_texts(train_reviews_cleaned)\n",
    "\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train_reviews_cleaned)\n",
    "val_sequences = keras_tokenizer.texts_to_sequences(val_reviews_cleaned)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_reviews_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Preiewing sequence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:17:45.657156Z",
     "start_time": "2020-03-09T08:17:45.654944Z"
    }
   },
   "outputs": [],
   "source": [
    "review_id = 1235"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:17:45.663687Z",
     "start_time": "2020-03-09T08:17:45.659197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"chiller start cold dark stormy night bus drop three passenger outside bus station young boy name mason jesse emery college professor dr howard conrow david wohl  woman name sharon phillips laurie pennington inside discover miss connect bus  strand night wait area find two people ronnie jim wolf  sleep woman name lindsay marjorie fitzsimmons currently terrify nightmarewhile swim indoor pool lindsay encounter  befriend guy name billy water jesse johnson next time lindsay see billy dive pool  seemingly disappear thin air surface shortly lindsay discover billy water die dive accident 5 year ago.lindsay wake & tell others nightmare, everyone else respond say suffer disturb dream recently & decide share pas time...next mason tell story & two friends, scott (david r. hamm) & jimmy (will tuckwiller), terrorise camp trip...then sharon whose story revolve around newsman name tom williams (thom delventhal) phone up, time tom front door actually turn vampire...it's ronnie's turn next & describe discover bring dead back life, unfortunately bring execute mass murderer nelson caulder (bradford boll) back homicidal life...finally dr. conrow tell tale two student bring ancient aztec war-god name ixpe (kimberly harbour) back life...then back bus station one last (predictable) twist...written, produce & direct daniel boyd chiller one worst horror anthology i've ever see & usually really like sub-genre. script boyd lack need film chiller work, see final twist come mile & story really lame. first one totally pointless & seem end & best thing anthology short snappy story round neat twist. second story predictable &, again, end without payoff. continue throughout chiller story deeply unsatisfying watch & reward so. character's & dialogue poorly written, story seem original idea & whole film totally sucks. least story last long & like idea behind link segments.director boyd wa obviously work low budget & shows. say want watch 15 odd minute short story set entirely within swim pool chiller you. story neither clever, scary sort tension build anything. say doe nice scene & surprise competence shine occasion. violence & gore wise much happen chillers, rip heart, decapitate head & bite hand gory gets.technically chiller poor stuff impress anyone. basic cinematography, bad music, cheap special effect & average production values. chiller also feature one worst close theme song ever, period. act also low standard.i sure lot effort wa put chiller low budget film & least filmmaker try give credit least, still stop think crap. similar anthology film like tale crypt (1972), asylum (1972), vault horror (1973), dr. terror's house horror (1965), creepshow (1982) & tale darkside: movie (1990) far superior chiller watch one instead.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_reviews_cleaned[review_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:17:45.667580Z",
     "start_time": "2020-03-09T08:17:45.665007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6633, 84, 953, 387, 9129, 209, 2148, 1099, 205, 3869, 903, 2148, 1435, 102, 245, 157, 5392, 2902, 1079, 1978, 784, 1777, 532, 77, 157, 4556, 5356, 6207, 855, 631, 232, 1509, 2148, 4234, 209, 378, 1177, 40, 46, 26, 1229, 3050, 919, 77, 157, 4844, 9547, 3306, 2195, 2277, 2427, 4844, 1372, 3651, 88, 157, 1423, 835, 2902, 2220, 294, 10, 4844, 7, 1423, 3373, 2427, 1409, 1633, 1417, 666, 2238, 3362, 4844, 631, 1423, 835, 351, 3373, 1478, 460, 55, 531, 4844, 1570, 82, 322, 1267, 225, 249, 3872, 25, 718, 927, 484, 896, 276, 907, 1317, 10, 294, 5392, 82, 16, 46, 2218, 945, 532, 1667, 1742, 3267, 988, 972, 1066, 4556, 556, 16, 1851, 101, 157, 713, 1467, 1282, 499, 10, 713, 864, 1036, 75, 96, 797, 1254, 96, 294, 730, 631, 188, 284, 65, 43, 418, 188, 1810, 1891, 2076, 2692, 8534, 2895, 65, 6219, 43, 329, 784, 82, 588, 46, 744, 188, 1926, 206, 446, 157, 65, 43, 1066, 65, 2148, 1435, 4, 136, 649, 441, 2792, 627, 223, 1701, 6633, 4, 160, 107, 5370, 647, 56, 7, 526, 19, 6, 2335, 420, 120, 280, 121, 3, 6633, 45, 7, 403, 441, 38, 1446, 16, 19, 758, 32, 4, 404, 1090, 44, 31, 53, 41, 5370, 248, 7290, 16, 1442, 2746, 441, 213, 16, 649, 586, 31, 126, 5875, 735, 407, 6633, 16, 1490, 5729, 13, 2346, 523, 2633, 302, 774, 2792, 16, 44, 115, 169, 140, 3, 404, 5831, 133, 16, 136, 113, 6, 169, 414, 2443, 67, 2, 436, 45, 348, 296, 3807, 25, 48, 13, 1050, 935, 119, 248, 16, 92, 967, 656, 2277, 2427, 6633, 560, 16, 971, 1011, 558, 306, 929, 494, 143, 25, 57, 243, 22, 255, 1730, 2721, 498, 547, 1568, 23, 122, 1126, 410, 6731, 257, 110, 274, 2018, 8754, 2191, 6633, 253, 439, 1081, 170, 990, 552, 27, 138, 600, 226, 171, 745, 259, 6073, 6633, 29, 278, 4, 160, 355, 435, 269, 56, 677, 39, 29, 348, 768, 251, 167, 66, 527, 2, 134, 6633, 348, 296, 3, 133, 573, 52, 33, 448, 133, 60, 366, 17, 521, 596, 5370, 3, 6, 588, 6915, 3644, 4694, 3644, 5815, 107, 4079, 784, 203, 107, 7602, 5790, 588, 1, 4252, 149, 1471, 6633, 13, 4, 217]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[review_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:17:45.902164Z",
     "start_time": "2020-03-09T08:17:45.669394Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "seq_lengths = [len(seq) for seq in train_sequences]\n",
    "plt.hist(seq_lengths, bins=50)\n",
    "plt.title(\"Sequence length in Train Data - {}\".format(np.mean(seq_lengths)))\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Amount of Reviews\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BRNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:34:08.228100Z",
     "start_time": "2020-03-09T08:34:08.220830Z"
    }
   },
   "outputs": [],
   "source": [
    "def swish(x):\n",
    "    \"\"\"Implementation of swish activation function.\"\"\"\n",
    "    return (K.sigmoid(x) * x)\n",
    "\n",
    "def train_rnn_model(train_sequences, train_targets, val_sequences, val_targets, embedding_size, \n",
    "                    batch_size, epochs, seq_length, dict_length):\n",
    "    \"\"\"Training BRNN classifier. Applies zero-id padding for too short sequences and shortens too \n",
    "    long sequences according to 'seq_length' parameter.\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train_sequences_pad = sequence.pad_sequences(train_sequences, maxlen=seq_length)\n",
    "    X_val_sequences_pad = sequence.pad_sequences(val_sequences, maxlen=seq_length)\n",
    "        \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(dict_length, embedding_size, input_length=seq_length))\n",
    "    model.add(Bidirectional(LSTM(32, return_sequences=True)))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(20, activation=\"swish\"))\n",
    "    model.add(Dropout(0.05))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "                    \n",
    "    model.fit(X_train_sequences_pad, train_targets, \n",
    "              validation_data=(X_val_sequences_pad, val_targets), \n",
    "              batch_size=batch_size, epochs=epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:35:05.478944Z",
     "start_time": "2020-03-09T08:34:09.671007Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 20s 975us/sample - loss: 0.4477 - accuracy: 0.7933 - val_loss: 0.3173 - val_accuracy: 0.8662\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 18s 887us/sample - loss: 0.2283 - accuracy: 0.9112 - val_loss: 0.3121 - val_accuracy: 0.8672\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 18s 897us/sample - loss: 0.1471 - accuracy: 0.9477 - val_loss: 0.3570 - val_accuracy: 0.8644\n"
     ]
    }
   ],
   "source": [
    "get_custom_objects().update({\"swish\": Activation(swish)})\n",
    "\n",
    "model = train_rnn_model(\n",
    "    train_sequences, train_targets_seq,\n",
    "    val_sequences, val_targets_seq,\n",
    "    embedding_size=128,\n",
    "    batch_size=100,\n",
    "    epochs=3,\n",
    "    seq_length=130,\n",
    "    dict_length=vocabulary_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-09T08:35:27.330341Z",
     "start_time": "2020-03-09T08:35:18.321956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.8514\n"
     ]
    }
   ],
   "source": [
    "X_test_seq = sequence.pad_sequences(test_sequences, maxlen=130)\n",
    "\n",
    "scores = model.evaluate(X_test_seq, test_targets_seq, verbose=0)\n",
    "print(\"Test accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-08T18:18:28.232341Z",
     "start_time": "2020-03-08T18:18:21.562Z"
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook, simple method for performing sentiment prediction for documents has been presented.\n",
    "\n",
    "Three models has been produced that gave similar results on the test dataset:\n",
    "- Multinominal Naive Bayes - **86.56%** correct predictions\n",
    "- XGBoost Clasifier - 86.54% correct predictions\n",
    "- BRNN - 85.14% correct predictions\n",
    "\n",
    "In such case, the best model would be `Multinominal Naive Bayes` as it is very simple model that doesn't require tuning. It has outperformed `XGBoost` and `BRNN` models that are more complicated and require larger training time.\n",
    "\n",
    "Good idea would be to perform analysis how each of those models is making mistakes and how does it react to data that it haven't seen.\n",
    "\n",
    "Apart from that, good thing would be to look at what percentage of tokens inside test and val datasets were not supported. It might give additional information whether current combination of parameters works well or not.\n",
    "\n",
    "Results depends on data representation - so how the data is preprocessed, cleaned, but also if each token is treated separately or maybe if it's represented in a context. There are many different NLP architectures that could improve the results. For example accordint to [this leaderboard](https://paperswithcode.com/sota/sentiment-analysis-on-imdb), some solutions are capable of achieving 97.7% accuracy.\n",
    "\n",
    "---\n",
    "There is additional intake from this notebook. The way corpus is processed with usage of multiprocessing module, allows to speed up building of Bag of Words and TF-IDF variants. Because whole corpus is tokenized and saved in form of list containing list of tokens, it can be didived into chunks that are processed separately and this gives speed up from ~25minutes (on current machine) to ~3 minutes. After that those lists can be passed to `CountVectorizer` that doesn't have to tokenize the data from scratch and can reuse the tokens with any type of filtering, experiment author wants. Creation of such `CountVectorizer` takes seconds. In case of current notebook ~1h was saved thanks to this operation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
