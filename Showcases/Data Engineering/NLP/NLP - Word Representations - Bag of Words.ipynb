{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a method of word representation for NLP related problems and data analysis called **Bag of Words**.\n",
    "\n",
    "It treats each document or text as an unordered collection. For example if the goal is to analyze tweets from Twitter then each separate tweet is a document in this case.\n",
    "\n",
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:40.196282Z",
     "start_time": "2020-03-01T00:19:40.193255Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The dog barks in the morning.\",\n",
    "    \"Over the sofa lies sleeping dog.\",\n",
    "    \"My dog name is Farell, it is very energetic.\",\n",
    "    \"The dog barks at the cars.\",\n",
    "    \"Cat dislikes vegetables.\",\n",
    "    \"Cats sleep during day and hunt during night.\",\n",
    "    \"Cats and dogs are not getting along. I prefer cats.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:41.021975Z",
     "start_time": "2020-03-01T00:19:40.198208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "for package in [\"punkt\", \"wordnet\", \"stopwords\"]:\n",
    "    nltk.download(package)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wodnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_document(document, stemmer=porter_stemmer, lemmatizer=wodnet_lemmatizer):\n",
    "    \"\"\"Noramlizes data by performing following steps:\n",
    "        1. Changing each word in corpus to lowercase.\n",
    "        2. Removing special characters and interpunction.\n",
    "        3. Dividing text into tokens.\n",
    "        4. Removing english stopwords.\n",
    "        5. Stemming words.\n",
    "        6. Lemmatizing words.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = document.lower()\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp)\n",
    "    temp = word_tokenize(temp)\n",
    "    temp = [t for t in temp if t not in stopwords.words(\"english\")]\n",
    "    temp = [porter_stemmer.stem(token) for token in temp]\n",
    "    temp = [lemmatizer.lemmatize(token) for token in temp]\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previeving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.666191Z",
     "start_time": "2020-03-01T00:19:41.023964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  ['dog', 'bark', 'morn']\n",
      "                   Over the sofa lies sleeping dog.  ->  ['sofa', 'lie', 'sleep', 'dog']\n",
      "       My dog name is Farell, it is very energetic.  ->  ['dog', 'name', 'farel', 'energet']\n",
      "                         The dog barks at the cars.  ->  ['dog', 'bark', 'car']\n",
      "                           Cat dislikes vegetables.  ->  ['cat', 'dislik', 'veget']\n",
      "       Cats sleep during day and hunt during night.  ->  ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  ['cat', 'dog', 'get', 'along', 'prefer', 'cat']\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document in corpus:\n",
    "    print(document.rjust(offset), \" -> \", normalize_document(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe what tokens are left from each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating the CountVectorizer model and removing English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.670191Z",
     "start_time": "2020-03-01T00:19:42.668028Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(tokenizer=normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Bag Of Words based on corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.691124Z",
     "start_time": "2020-03-01T00:19:42.671714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function normalize_document at 0x10d237b70>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing tokens in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.695798Z",
     "start_time": "2020-03-01T00:19:42.692737Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along', 'bark', 'car', 'cat', 'day', 'dislik', 'dog', 'energet', 'farel', 'get', 'hunt', 'lie', 'morn', 'name', 'night', 'prefer', 'sleep', 'sofa', 'veget']\n"
     ]
    }
   ],
   "source": [
    "print(bow.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see te size of the bag is 16 as there are 16 tokens inside of it. Because of that each sentence will be represented with vector of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.712626Z",
     "start_time": "2020-03-01T00:19:42.697281Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_vectorized = bow.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.726257Z",
     "start_time": "2020-03-01T00:19:42.716029Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  [0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "                   Over the sofa lies sleeping dog.  ->  [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0]\n",
      "       My dog name is Farell, it is very energetic.  ->  [0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0]\n",
      "                         The dog barks at the cars.  ->  [0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                           Cat dislikes vegetables.  ->  [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "       Cats sleep during day and hunt during night.  ->  [0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  [1 0 0 2 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document, document_vector in zip(corpus, corpus_vectorized.toarray()):\n",
    "    print(document.rjust(offset), \" -> \", document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such vectors are now representing sentences in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is desired to don't include counts of words but rather whether word from dictionary/bag is in sentence or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.751580Z",
     "start_time": "2020-03-01T00:19:42.730131Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_ohe = CountVectorizer(tokenizer=normalize_document, binary=True)\n",
    "corpus_vectorized_ohe = bow_ohe.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.761827Z",
     "start_time": "2020-03-01T00:19:42.753647Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  [0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      "                   Over the sofa lies sleeping dog.  ->  [0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0]\n",
      "       My dog name is Farell, it is very energetic.  ->  [0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0]\n",
      "                         The dog barks at the cars.  ->  [0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                           Cat dislikes vegetables.  ->  [0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "       Cats sleep during day and hunt during night.  ->  [0 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0]\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  [1 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document, document_vector in zip(corpus, corpus_vectorized_ohe.toarray()):\n",
    "    print(document.rjust(offset), \" -> \", document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngrams allows to create tokens from group of words to get more information about their context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:11:47.458593Z",
     "start_time": "2020-03-01T00:11:47.454929Z"
    }
   },
   "source": [
    "- ngram range (1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.782614Z",
     "start_time": "2020-03-01T00:19:42.764560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along', 'along prefer', 'bark', 'bark car', 'bark morn', 'car', 'cat', 'cat dislik', 'cat dog', 'cat sleep', 'day', 'day hunt', 'dislik', 'dislik veget', 'dog', 'dog bark', 'dog get', 'dog name', 'energet', 'farel', 'farel energet', 'get', 'get along', 'hunt', 'hunt night', 'lie', 'lie sleep', 'morn', 'name', 'name farel', 'night', 'prefer', 'prefer cat', 'sleep', 'sleep day', 'sleep dog', 'sofa', 'sofa lie', 'veget']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_12 = CountVectorizer(tokenizer=normalize_document, ngram_range=(1,2))\n",
    "bow_ngram_12.fit(corpus)\n",
    "print(bow_ngram_12.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram range (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.804588Z",
     "start_time": "2020-03-01T00:19:42.784571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along prefer', 'bark car', 'bark morn', 'cat dislik', 'cat dog', 'cat sleep', 'day hunt', 'dislik veget', 'dog bark', 'dog get', 'dog name', 'farel energet', 'get along', 'hunt night', 'lie sleep', 'name farel', 'prefer cat', 'sleep day', 'sleep dog', 'sofa lie']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_22 = CountVectorizer(tokenizer=normalize_document, ngram_range=(2,2))\n",
    "bow_ngram_22.fit(corpus)\n",
    "print(bow_ngram_22.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram range (1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.827462Z",
     "start_time": "2020-03-01T00:19:42.806829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along', 'along prefer', 'along prefer cat', 'bark', 'bark car', 'bark morn', 'car', 'cat', 'cat dislik', 'cat dislik veget', 'cat dog', 'cat dog get', 'cat sleep', 'cat sleep day', 'day', 'day hunt', 'day hunt night', 'dislik', 'dislik veget', 'dog', 'dog bark', 'dog bark car', 'dog bark morn', 'dog get', 'dog get along', 'dog name', 'dog name farel', 'energet', 'farel', 'farel energet', 'get', 'get along', 'get along prefer', 'hunt', 'hunt night', 'lie', 'lie sleep', 'lie sleep dog', 'morn', 'name', 'name farel', 'name farel energet', 'night', 'prefer', 'prefer cat', 'sleep', 'sleep day', 'sleep day hunt', 'sleep dog', 'sofa', 'sofa lie', 'sofa lie sleep', 'veget']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_13 = CountVectorizer(tokenizer=normalize_document, ngram_range=(1,3))\n",
    "bow_ngram_13.fit(corpus)\n",
    "print(bow_ngram_13.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ngram range (2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T00:19:42.851191Z",
     "start_time": "2020-03-01T00:19:42.831154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['along prefer', 'along prefer cat', 'bark car', 'bark morn', 'cat dislik', 'cat dislik veget', 'cat dog', 'cat dog get', 'cat sleep', 'cat sleep day', 'day hunt', 'day hunt night', 'dislik veget', 'dog bark', 'dog bark car', 'dog bark morn', 'dog get', 'dog get along', 'dog name', 'dog name farel', 'farel energet', 'get along', 'get along prefer', 'hunt night', 'lie sleep', 'lie sleep dog', 'name farel', 'name farel energet', 'prefer cat', 'sleep day', 'sleep day hunt', 'sleep dog', 'sofa lie', 'sofa lie sleep']\n"
     ]
    }
   ],
   "source": [
    "bow_ngram_23 = CountVectorizer(tokenizer=normalize_document, ngram_range=(2,3))\n",
    "bow_ngram_23.fit(corpus)\n",
    "print(bow_ngram_23.get_feature_names())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
