{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows a method of word representation for NLP related problems and data analysis called **TF-IDF** which is a short of term frequencyâ€“inverse document frequency.\n",
    "\n",
    "It is an improved concept of **Bag of Words** which treats each word equaly. **TF-IDF** is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "**TF-IDF** equation:\n",
    "\n",
    "$$ tf_{i,j} = \\frac{n_{i,j}}{\\sum_k n_{k,j}} $$ \n",
    "\n",
    "$$ idf(w) = \\mbox{log} \\frac{N}{df_i} $$\n",
    "\n",
    "$$ w_{i,j} = tf_{i,j} \\times \\mbox{log} \\frac{N}{df_i} $$\n",
    "\n",
    "where:\n",
    "- $tf$ - term frequecny\n",
    "- $idf$ - inverse document frequency\n",
    "- $w$ - tfidf\n",
    "\n",
    "- $i$ - index of term\n",
    "- $j$ - index of document\n",
    "- $k$ - number of terms in document\n",
    "- $N$ - corpus length (number of documents)\n",
    "- $df_i$ - number of documents containing term i\n",
    "\n",
    "### 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:56:11.268916Z",
     "start_time": "2020-03-01T01:56:11.265950Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The dog barks in the morning.\",\n",
    "    \"Over the sofa lies sleeping dog.\",\n",
    "    \"My dog name is Farell, it is very energetic.\",\n",
    "    \"The dog barks at the cars.\",\n",
    "    \"Cat dislikes vegetables.\",\n",
    "    \"Cats sleep during day and hunt during night.\",\n",
    "    \"Cats, dogs and elephants are animals.\",\n",
    "    \"Dogs can run quickly.\",\n",
    "    \"My favourite animals are dogs.\",\n",
    "    \"There are many different animals in the world.\",\n",
    "    \"When I buy a house I will also adopt two cats.\",\n",
    "    \"On cat is black and the other cat is white.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cleaning corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:56:11.779620Z",
     "start_time": "2020-03-01T01:56:11.772449Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kamilkrzyk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import nltk\n",
    "for package in [\"punkt\", \"wordnet\", \"stopwords\"]:\n",
    "    nltk.download(package)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "wodnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize_document(document, stemmer=porter_stemmer, lemmatizer=wodnet_lemmatizer):\n",
    "    \"\"\"Noramlizes data by performing following steps:\n",
    "        1. Changing each word in corpus to lowercase.\n",
    "        2. Removing special characters and interpunction.\n",
    "        3. Dividing text into tokens.\n",
    "        4. Removing english stopwords.\n",
    "        5. Stemming words.\n",
    "        6. Lemmatizing words.\n",
    "    \"\"\"\n",
    "    \n",
    "    temp = document.lower()\n",
    "    temp = re.sub(r\"[^a-zA-Z0-9]\", \" \", temp)\n",
    "    temp = word_tokenize(temp)\n",
    "    temp = [t for t in temp if t not in stopwords.words(\"english\")]\n",
    "    temp = [porter_stemmer.stem(token) for token in temp]\n",
    "    temp = [lemmatizer.lemmatize(token) for token in temp]\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previeving results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:47.940637Z",
     "start_time": "2020-03-01T01:22:46.362804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  ['dog', 'bark', 'morn']\n",
      "                   Over the sofa lies sleeping dog.  ->  ['sofa', 'lie', 'sleep', 'dog']\n",
      "       My dog name is Farell, it is very energetic.  ->  ['dog', 'name', 'farel', 'energet']\n",
      "                         The dog barks at the cars.  ->  ['dog', 'bark', 'car']\n",
      "                           Cat dislikes vegetables.  ->  ['cat', 'dislik', 'veget']\n",
      "       Cats sleep during day and hunt during night.  ->  ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  ['cat', 'dog', 'get', 'along', 'prefer', 'cat']\n",
      "              Cats, dogs and elephants are animals.  ->  ['cat', 'dog', 'eleph', 'anim']\n",
      "                              Dogs can run quickly.  ->  ['dog', 'run', 'quickli']\n",
      "                     My favourite animals are dogs.  ->  ['favourit', 'anim', 'dog']\n",
      "     There are many different animals in the world.  ->  ['mani', 'differ', 'anim', 'world']\n",
      "     When I buy a house I will also adopt two cats.  ->  ['buy', 'hous', 'also', 'adopt', 'two', 'cat']\n",
      "        On cat is black and the other cat is white.  ->  ['cat', 'black', 'cat', 'white']\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document in corpus:\n",
    "    print(document.rjust(offset), \" -> \", normalize_document(document))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to observe what tokens are left from each sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Creating Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiating the CountVectorizer model and removing English stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:47.945358Z",
     "start_time": "2020-03-01T01:22:47.942914Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow = CountVectorizer(tokenizer=normalize_document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Bag Of Words based on corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:47.974349Z",
     "start_time": "2020-03-01T01:22:47.946735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function normalize_document at 0x107080b70>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.fit(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previewing tokens in the bag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:47.978448Z",
     "start_time": "2020-03-01T01:22:47.975867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adopt', 'along', 'also', 'anim', 'bark', 'black', 'buy', 'car', 'cat', 'day', 'differ', 'dislik', 'dog', 'eleph', 'energet', 'farel', 'favourit', 'get', 'hous', 'hunt', 'lie', 'mani', 'morn', 'name', 'night', 'prefer', 'quickli', 'run', 'sleep', 'sofa', 'two', 'veget', 'white', 'world']\n"
     ]
    }
   ],
   "source": [
    "print(bow.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is possible to see te size of the bag is 16 as there are 16 tokens inside of it. Because of that each sentence will be represented with vector of size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:47.999082Z",
     "start_time": "2020-03-01T01:22:47.979646Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus_vectorized = bow.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.011551Z",
     "start_time": "2020-03-01T01:22:48.001285Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      The dog barks in the morning.  ->  [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                   Over the sofa lies sleeping dog.  ->  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
      "       My dog name is Farell, it is very energetic.  ->  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      "                         The dog barks at the cars.  ->  [0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                           Cat dislikes vegetables.  ->  [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      "       Cats sleep during day and hunt during night.  ->  [0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
      "Cats and dogs are not getting along. I prefer cats.  ->  [0 1 0 0 0 0 0 0 2 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      "              Cats, dogs and elephants are animals.  ->  [0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "                              Dogs can run quickly.  ->  [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "                     My favourite animals are dogs.  ->  [0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "     There are many different animals in the world.  ->  [0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      "     When I buy a house I will also adopt two cats.  ->  [1 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      "        On cat is black and the other cat is white.  ->  [0 0 0 0 0 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "offset = max(map(len, corpus))\n",
    "for document, document_vector in zip(corpus, corpus_vectorized.toarray()):\n",
    "    print(document.rjust(offset), \" -> \", document_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such vectors are now representing sentences in corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating TF-IDF values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:09:42.687503Z",
     "start_time": "2020-03-01T01:09:42.683642Z"
    }
   },
   "source": [
    "Initializing Tfidf transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.017580Z",
     "start_time": "2020-03-01T01:22:48.014618Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.025403Z",
     "start_time": "2020-03-01T01:22:48.020240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_transformer.fit(corpus_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising frequencies per term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.031757Z",
     "start_time": "2020-03-01T01:22:48.027119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.94591015, 2.94591015, 2.94591015, 2.25276297, 2.54044504,\n",
       "       2.94591015, 2.94591015, 2.94591015, 1.69314718, 2.94591015,\n",
       "       2.94591015, 2.94591015, 1.44183275, 2.94591015, 2.94591015,\n",
       "       2.94591015, 2.94591015, 2.94591015, 2.94591015, 2.94591015,\n",
       "       2.94591015, 2.94591015, 2.94591015, 2.94591015, 2.94591015,\n",
       "       2.94591015, 2.94591015, 2.94591015, 2.54044504, 2.94591015,\n",
       "       2.94591015, 2.94591015, 2.94591015, 2.94591015])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.049714Z",
     "start_time": "2020-03-01T01:22:48.033492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     adopt  :  2.9459101490553135\n",
      "     along  :  2.9459101490553135\n",
      "      also  :  2.9459101490553135\n",
      "      anim  :  2.252762968495368\n",
      "      bark  :  2.540445040947149\n",
      "     black  :  2.9459101490553135\n",
      "       buy  :  2.9459101490553135\n",
      "       car  :  2.9459101490553135\n",
      "       cat  :  1.6931471805599454\n",
      "       day  :  2.9459101490553135\n",
      "    differ  :  2.9459101490553135\n",
      "    dislik  :  2.9459101490553135\n",
      "       dog  :  1.4418327522790393\n",
      "     eleph  :  2.9459101490553135\n",
      "   energet  :  2.9459101490553135\n",
      "     farel  :  2.9459101490553135\n",
      "  favourit  :  2.9459101490553135\n",
      "       get  :  2.9459101490553135\n",
      "      hous  :  2.9459101490553135\n",
      "      hunt  :  2.9459101490553135\n",
      "       lie  :  2.9459101490553135\n",
      "      mani  :  2.9459101490553135\n",
      "      morn  :  2.9459101490553135\n",
      "      name  :  2.9459101490553135\n",
      "     night  :  2.9459101490553135\n",
      "    prefer  :  2.9459101490553135\n",
      "   quickli  :  2.9459101490553135\n",
      "       run  :  2.9459101490553135\n",
      "     sleep  :  2.540445040947149\n",
      "      sofa  :  2.9459101490553135\n",
      "       two  :  2.9459101490553135\n",
      "     veget  :  2.9459101490553135\n",
      "     white  :  2.9459101490553135\n",
      "     world  :  2.9459101490553135\n"
     ]
    }
   ],
   "source": [
    "for term, freq in zip(bow.get_feature_names(), tf_idf_transformer.idf_):\n",
    "    print(term.rjust(10), \" : \", freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising frequency for document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.055874Z",
     "start_time": "2020-03-01T01:22:48.052021Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_docs = tf_idf_transformer.transform(corpus_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.066645Z",
     "start_time": "2020-03-01T01:22:48.058019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 34)\n",
      "[[0.         0.         0.         0.         0.61235761 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.34754433 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.71009232 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.28336938 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.57897196 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.49928422 0.57897196\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.27192757 0.         0.5555944  0.5555944  0.         0.\n",
      "  0.         0.         0.         0.         0.         0.5555944\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.61235761 0.\n",
      "  0.         0.71009232 0.         0.         0.         0.\n",
      "  0.34754433 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.37650117 0.         0.         0.65507514\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.65507514 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.28475055 0.49543805 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.49543805 0.         0.         0.         0.\n",
      "  0.49543805 0.         0.         0.         0.42724763 0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.46824802 0.         0.         0.         0.\n",
      "  0.         0.         0.53824643 0.         0.         0.\n",
      "  0.22917716 0.         0.         0.         0.         0.46824802\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.46824802 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.52096292 0.         0.\n",
      "  0.         0.         0.39154891 0.         0.         0.\n",
      "  0.33343117 0.68125674 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32705086 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.66822067 0.66822067 0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.56616724 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.36236323 0.         0.         0.         0.74036987 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.40389157 0.         0.\n",
      "  0.         0.         0.         0.         0.52816399 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.52816399 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.52816399]\n",
      " [0.4331346  0.         0.4331346  0.         0.         0.\n",
      "  0.4331346  0.         0.24894195 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4331346  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.4331346  0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.54871162\n",
      "  0.         0.         0.63073854 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.54871162 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_docs.toarray().shape)\n",
    "print(tfidf_docs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-01T01:22:48.272654Z",
     "start_time": "2020-03-01T01:22:48.069165Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document id.0: The dog barks in the morning.\n",
      "Tokens: ['dog', 'bark', 'morn']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.61235761]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.34754433]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.71009232]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.1: Over the sofa lies sleeping dog.\n",
      "Tokens: ['sofa', 'lie', 'sleep', 'dog']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.28336938]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.57897196]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.49928422]\n",
      "      sofa  :  [0.57897196]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.2: My dog name is Farell, it is very energetic.\n",
      "Tokens: ['dog', 'name', 'farel', 'energet']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.27192757]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.5555944]\n",
      "     farel  :  [0.5555944]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.5555944]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.3: The dog barks at the cars.\n",
      "Tokens: ['dog', 'bark', 'car']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.61235761]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.71009232]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.34754433]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.4: Cat dislikes vegetables.\n",
      "Tokens: ['cat', 'dislik', 'veget']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.37650117]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.65507514]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.65507514]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.5: Cats sleep during day and hunt during night.\n",
      "Tokens: ['cat', 'sleep', 'day', 'hunt', 'night']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.28475055]\n",
      "       day  :  [0.49543805]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.49543805]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.49543805]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.42724763]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.6: Cats and dogs are not getting along. I prefer cats.\n",
      "Tokens: ['cat', 'dog', 'get', 'along', 'prefer', 'cat']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.46824802]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.53824643]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.22917716]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.46824802]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.46824802]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.7: Cats, dogs and elephants are animals.\n",
      "Tokens: ['cat', 'dog', 'eleph', 'anim']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.52096292]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.39154891]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.33343117]\n",
      "     eleph  :  [0.68125674]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.8: Dogs can run quickly.\n",
      "Tokens: ['dog', 'run', 'quickli']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.32705086]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.66822067]\n",
      "       run  :  [0.66822067]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.9: My favourite animals are dogs.\n",
      "Tokens: ['favourit', 'anim', 'dog']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.56616724]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.36236323]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.74036987]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.10: There are many different animals in the world.\n",
      "Tokens: ['mani', 'differ', 'anim', 'world']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.40389157]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.52816399]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mani  :  [0.52816399]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.52816399]\n",
      "\n",
      " ------------------\n",
      "Document id.11: When I buy a house I will also adopt two cats.\n",
      "Tokens: ['buy', 'hous', 'also', 'adopt', 'two', 'cat']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.4331346]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.4331346]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.]\n",
      "       buy  :  [0.4331346]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.24894195]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.4331346]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.4331346]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n",
      "Document id.12: On cat is black and the other cat is white.\n",
      "Tokens: ['cat', 'black', 'cat', 'white']\n",
      "\n",
      " -- TF IDF Values for words in dictionary:\n",
      "     adopt  :  [0.]\n",
      "     along  :  [0.]\n",
      "      also  :  [0.]\n",
      "      anim  :  [0.]\n",
      "      bark  :  [0.]\n",
      "     black  :  [0.54871162]\n",
      "       buy  :  [0.]\n",
      "       car  :  [0.]\n",
      "       cat  :  [0.63073854]\n",
      "       day  :  [0.]\n",
      "    differ  :  [0.]\n",
      "    dislik  :  [0.]\n",
      "       dog  :  [0.]\n",
      "     eleph  :  [0.]\n",
      "   energet  :  [0.]\n",
      "     farel  :  [0.]\n",
      "  favourit  :  [0.]\n",
      "       get  :  [0.]\n",
      "      hous  :  [0.]\n",
      "      hunt  :  [0.]\n",
      "       lie  :  [0.]\n",
      "      mani  :  [0.]\n",
      "      morn  :  [0.]\n",
      "      name  :  [0.]\n",
      "     night  :  [0.]\n",
      "    prefer  :  [0.]\n",
      "   quickli  :  [0.]\n",
      "       run  :  [0.]\n",
      "     sleep  :  [0.]\n",
      "      sofa  :  [0.]\n",
      "       two  :  [0.]\n",
      "     veget  :  [0.]\n",
      "     white  :  [0.54871162]\n",
      "     world  :  [0.]\n",
      "\n",
      " ------------------\n"
     ]
    }
   ],
   "source": [
    "for doc_id in range(len(corpus)):\n",
    "    print(\"Document id.{}: {}\".format(doc_id, corpus[doc_id]))\n",
    "    print(\"Tokens: {}\".format(normalize_document(corpus[doc_id])))\n",
    "    print(\"\\n -- TF IDF Values for words in dictionary:\")\n",
    "    for term, freq in zip(bow.get_feature_names(), tfidf_docs[doc_id].T.toarray()):\n",
    "        print(term.rjust(10), \" : \", freq)\n",
    "    print(\"\\n ------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
